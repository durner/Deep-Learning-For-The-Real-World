Problem 14: I implemented the gradient descent neural network with the help of climin's optimizers. Accordingly, I could easily change my implementation to rmsprop. For the code see neural_network.py, function problem 14.

Problem 14 Bonus: I also implemented the training without climin and gradient descent. There I used also l1 regularisation for improving my results. See function problem_14_bonus for the code.

Problem 15: Using standard gradient descent and tanh, I got an error of about 2.2%. We can see that we can learn the representation of MNIST way better with a neural network than pure Logistic Regression. I used a batch size of 100 and run at most 30 epochs (but implemented early stopping).

Problem 16: First of all it is clear that we need to have different starting values for the hidden units, otherwise they might learn the same function. Therefore, each and every of the activation function should use small random values. According to Glorot and Bengio it makes sense to use the normalized initialization especially important for tanh networks. Furthermore, they and Deep Learning Tutorial suggest to use 4 times the values for sigmoid units than for tanh. [Glorot, Xavier, and Yoshua Bengio. "Understanding the difficulty of training deep feedforward neural networks." International conference on artificial intelligence and statistics. 2010.]. Testing this paper results, I could confirm that tanh did a slightly better accuracy on test with the normalized initialization (without multiplication) and sigmoid does better slighlty better with four times increased normalized initialization (but very similar results). One could think about what values those functions returns. Looking how those function look like (https://plot.ly/~votingelephant/17.png), one could argue that this comes from the different steepness of both functions. For ReLu I tried the initialization function for the weights of the tanh and sigmoid activation and achieved my best overall NN results with ReLu and the this initialization values. Furthermore, ReLu nets seem to be lesser dependent on the initialization than tanh or sigmoid nets. [He, Kaiming, et al. "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification." Proceedings of the IEEE International Conference on Computer Vision. 2015.] In my case both the value generation function for tanh and sigmoid result in similar error curves for Graphically, I included also the error plots in the repository with changed activation function. Those plots have "_test" as subsequence in their name.

Problem 17 & 18: I used the gradient descent algorithm for computing the plots.

Problem 19: With augumented MNIST dataset (described in the logreg README) I can achieve wtih Gradient Descent and as activation function relu an error on my test set of about 1.7%. See repflds_optimized.png and errors_optimized.png.
