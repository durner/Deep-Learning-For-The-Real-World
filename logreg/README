Problem 8: see logreg.py

Problem 9: Evaluation with the standart stochastic gradient descent got an test error result of about 7.6% on MNIST without augumented training data. Good working parameters for batch_size and learning rate were batch_size = 600 and learning rate = 0.13. Since I later on tested the same with augmented data, that included every image twice but rotated between +-5 degrees, I am going to show in the following all plots with the enlarged train set.

Problem 10: In the first step I just tried to reproduce my result with climin's Gradient Descent algorithm. After having this running I tried some other algorithms and I came to adadelta. One thing all algorithms had in common was that the overall missclassification rate was not changing a lot. Hence, most of the algorithm just differed how long it takes until you reach this "error threshold". For example the AdaDelta algorithm could find a good classification configuration after fewer interations without increasing the error.

Problem 11: For the regular thenao w/o climin implementation see the repflds.png file. For climin's Gradient Descent see repflds_sgd.png and for climin's adadelta repflds_adadelta.png.

Problem 12: Since it is unncessary to still train the network even though there are no / minor effects anymore, I implemented early stopping if we don't get any improvements on validation after 20 epochs anymore or reaches 300 epochs in total (okay this value seems a little high already but I also wanted to see a graph enough to argue). For the gradient descent version w/o climin we can see that the algorithm stops after 175 epochs already. One could argue that its actually enogug to stop the learning process after fewer iterations. Looking at the graph we can see that the validation error doesn' really improve after 120 epochs anymore, neither does the training error. Also the test error is already stable and doesn't change a lot anymore (beeing scientifically correct we shouldn't look at the test results :).

Problem 13: As already described I choose to augement the training data such that for each image I included a randomly shifted (read rotated) image of about +-5 degrees. With this I got my best result for Logistic Regression of about 7.3% on the test data.

Bonus Problem: Since we were looking at the test error, we were more or less improving our train set and parameters on the test results. Actually, this shouldn't be done since we might just have found a good working solution for this specific test set. Therefore, a lot of competition only allow you a certain amount of queries to the test set, that you cannot cheat by optimizing on the test set. Best example is the AI test were Baidu cheated by "having multiple" teams.
