Problem 20: See pca.py

Problem 21: See files scatterplotMNIST.png and scatterplotCIFAR.png

Problem 22: I implemented both gradient descent and rmsprop. For stochastic gradient descent I wrote pure theano, whereas for rmsprop I used the climin optimizer. Those implementation can be found in train and train_climin. For the following tasks I sticked with the climin rmsprop implementation.

Problem 23: The increase in the hidden units results into more sharp representation of the numbers. This is actually expected since with lower hidden units less information can be represented. Furthermore, with a higher amount of hidden units, it is important to have a sparsity constraint to avoid that the reconstructed picture is not just the original one. With this sparsity constraint each of the hidden units are forced to have only a small activation output otherwise the loss will grow.

Problem 24: I did test some different combinations of lambdas and hidden unit size. Therefore, I stored several images for autoencoderrec.png. The string suffix describes the optimizer used, the first number denotes the number of hidden units and the second one the lamda param. As expected a lower value of lambda does reduce the non-sparsity loss. Hence, most of the hidden units have an activation output for some features of the numbers. Therefore the outputs are sharper (almost like the real input) with a lower lambda param and high hidden values. Higher lambda brings sparse encoding that means that fewer hidden units have more general output of the images and therefore the outputs get less sharp.

Problem 25: I also stored several images for autoencoderfilter.png. The string suffix describes the optimizer used, the first number denotes the number of hidden units and the second one the lamda param. We can easily see the effect of increasing lambda. Most of the filters in the sets with more hidden units are having an activation of roughly 0 and some have some general representation of the correspeonding numbers, whereas with a lower lambda the filters have specialized activation of parts of the number. The lower the number of hidden units, the less the sparsity constraint drives the overall cost. Hence, most hidden units, also with high lambda, are activated to represent some generalization of a specific number.

Problem 26: 

Bonus Problem: I also added KL Divergence, for the results of the filter and reconstructions just look at the files with the substring _kl_.
