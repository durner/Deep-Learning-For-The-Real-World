Problem 20: See pca.py

Problem 21: See files scatterplotMNIST.png and scatterplotCIFAR.png

Problem 22: I implemented both gradient descent and rmsprop. For stochastic gradient descent I wrote pure theano, whereas for rmsprop I used the climin optimizer. Those implementation can be found in train and train_climin. For the following tasks I sticked with the climin rmsprop implementation since the auto encoder was easier to train with it. See auto_encoder.py

Problem 23: The increase in the hidden units w/o adding sparsity results into more sharp representation of the numbers. This is actually expected since with lower hidden units less information can be represented. Furthermore, with a higher amount of hidden units, it is important to have a sparsity constraint to avoid that the reconstructed picture is not just the original one. With this sparsity constraint many units don't have an activation and some represent some general representation of a number.

Problem 24: I did test some different combinations of lambdas and hidden unit size. Therefore, I stored several images for autoencoderrec.png. The string suffix describes the optimizer used, the first number denotes the number of hidden units and the second one the lamda param. As expected a lower value of lambda does reduce the non-sparsity loss. Hence, most of the hidden units have an activation output for some features of the numbers. Therefore the outputs with a lower lambda param and high hidden values are sharper (almost like the real input). Higher lambda brings sparse encoding that means that fewer hidden units have more general output of the images and therefore the outputs get less sharp since fewer units are used to represent the number.

Problem 25: I also stored several images for autoencoderfilter.png. The string suffix describes the optimizer used, the first number denotes the number of hidden units and the second one the lamda param. We can easily see the effect of increasing lambda. Most of the filters in the sets with more hidden units are having an activation of roughly 0 and some have some general representation of the corresponding numbers, whereas with a lower lambda the filters have specialized activation of parts of the number. The lower the number of hidden units, the less the sparsity constraint drives the overall cost. Hence, most hidden units, also with high lambda, are activated to represent some generalization of a specific number. We can see that we learn with a high number units and low sparsity each possible image part one by one, whereas only with the sparsity constrain the generalization of the input is achieved.

Problem 26: Instead of learning pixels or partial features of numbers, a sparse encoding means that I am only having a few units that represent one class of numbers (e.g. a few units that represent 3's). Hence, we learn a few general representation of the numbers and now represented the inputs as a combination of those learned general number representation.
