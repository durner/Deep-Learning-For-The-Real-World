Problem 20: See pca.py

Problem 21: See files scatterplotMNIST.png and scatterplotCIFAR.png

Problem 22: I implemented both gradient descent and rmsprop. For stochastic gradient descent I wrote pure theano, whereas for rmsprop I used the climin optimizer. Those implementation can be found in train and train_climin. For the following tasks I sticked with gradient descent and did only test the hidden value with SGD.

Problem 23: The increase in the hidden units results into more sharp representation of the numbers. This is actually expected since with lower hidden units less information can be stored. Furthermore, having a higher amount of hidden units, it is important to have a higher sparsity constraint to avoid that the reconstructed picture is not just the original one. With this sparsity constraint each of the hidden units are forced to have only a small activation output otherwise the loss will grow. As noted frivolously, this is implemented for the gradient descent w/o climin.

Problem 24: I did test some different combinations of lambdas and hidden unit size. Therefore, I stored several images for autoencoderrec.png. The first number denotes the number of hidden units and the second one the lamda param. As expected a lower value if lambda does reduce the non-sparsity loss. Hence, the hidden units might have a higher activation output, therefore the outputs are sharper with a lower lambda param.

Problem 25: I stored several images for autoencoderfilter.png. The first number denotes the number of hidden units and the second one the lamda param. We can see that the contrast is higher for lower lambda params. A higher contrast means that we have more absolute higher values. This was expected since the filter does have a smaller loss from the regularization with smaller lambda values. Therefore, the absolute values can be higher since this doesn't effect the loss cost as much as with a higher lambda.

Problem 26: This means that I can learn the important features of the digits with my sparse autoencoder. Even though I might have many neurons, with the sparsity constrains the overall activation is not too big since this would increase the loss. Therefore, the sparse encoding tries to find the most important features and just have an activation there. Hence, we can learn get a new output of the data set that is represented by the most important features. Hence, we can easier learn this sparse encoded MNIST dataset.
